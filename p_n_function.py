# -*- coding: utf-8 -*-
"""p-n_function

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z-FlQQqIaWimFcBrSyojvL5Q_T8du8Is
"""

# importing the essential packages for text processing and managing the dataFrame
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# paste the file name on which you are working
dataset = pd.read_csv("file_name",delimiter='\t',quoting=3)
X=dataset.iloc[:,0]
Y=dataset.iloc[:,1].values

#basic text cleaning
a=[]
ps=PorterStemmer()
for i in range(len(X)):
  r=re.sub('[^a-zA-Z]',' ',X[i])
  r=r.lower()
  r=[word for word in word_tokenize(X[i]) if not word in set(stopwords.words('english'))]
  r=' '.join(r)
  a.append(r)

# producing text data and train data
from sklearn.model_selection import train_test_split
cv=CountVectorizer(max_features=1500)
X=cv.fit_transform(a).toarray()
x_train,y_train,x_test,y_test=train_test_split(X,Y,test_size=0.2,random_state=0)

# algorithm based on mean estimation of the data

# wights imply the amount of magnitude with which the word contributes to the meaning of the sentence
weights=np.zeros((1,x_train.shape[0]))

# impact gives the actual summation of the sentiment of the sentence whenever the word appears
impact=np.zeros((1,x_train.shape[0]))

# number of times the word has appeared in entire text
count=np.zeros((1,x_train.shape[0]))

# the algorithm just means each feature(word) weightage in computation of the sentiment of the sentence
for i in range(x_train.shape[0]):
  for j in range(x_train.shape[1]):
    if (x_train[i][j]==1):
      count[j]=count[j]+1
      impact=impact+y_train[i]
      weights=impact[j]/count[j]

# a parameter on which we decide whether a sentence is in positive or negative sense
threshold=np.mean(weights)

# basic calculation of results which tells number f success and number of errors in prediction 
results=[]
for i in range(x_test.shape[0]):
  summation=0
  for j in range(x_test.shape[0]):
    summation=summation+weights[j]
  if (summation>threshold):
    c=1
  else:
    c=0
  if (c==y_test[i]):
    results.append(1)
  else:
    results.append(0)
accuracy=(sum(results)/len(results))*100
print("accuracy of the classifier is ",accuracy,sep='')

# thank you for reading the code